{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886a7ef1-297d-4373-b70e-438a6f9eda15",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c3e1c9-e4e9-4ede-b94d-2f606677561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48dd9ff0-4ffc-4cf2-93a1-ce24c5b30032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_valid = pd.read_csv('data/valid.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac33f72c-1bb4-4bd7-9a6a-b44aaf6ea947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I grew up (b. 1965) watching and loving the Th...      0\n",
      "1  When I put this movie in my DVD player, and sa...      0\n",
      "2  Why do people who do not know what a particula...      0\n",
      "3  Even though I have great interest in Biblical ...      0\n",
      "4  Im a die hard Dads Army fan and nothing will e...      1\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6cd07a-2d0d-461c-8cd7-aa26260d980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  It's been about 14 years since Sharon Stone aw...      0\n",
      "1  someone needed to make a car payment... this i...      0\n",
      "2  The Guidelines state that a comment must conta...      0\n",
      "3  This movie is a muddled mish-mash of clich√©s f...      0\n",
      "4  Before Stan Laurel became the smaller half of ...      0\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2638c197-b15e-4002-b816-13e23a4cbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I always wrote this series off as being a comp...      0\n",
      "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
      "2  This movie was so poorly written and directed ...      0\n",
      "3  The most interesting thing about Miryang (Secr...      1\n",
      "4  when i first read about \"berlin am meer\" i did...      0\n"
     ]
    }
   ],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8100444-8f30-4a78-bd48-653f82522f53",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* Change text to lower case\n",
    "* Remove any urls\n",
    "* Remove punctuation\n",
    "* Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660879ae-67c8-4518-9769-7401c915ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "df_train['text'] = df_train['text'].str.lower()\n",
    "df_valid['text'] = df_valid['text'].str.lower()\n",
    "df_test['text'] = df_test['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b208ca78-e9b0-4af4-a828-d227cfa69dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLS\n",
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    return url.sub(r'', text)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_urls)\n",
    "df_valid['text'] = df_valid['text'].apply(remove_urls)\n",
    "df_test['text'] = df_test['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9420b5d4-b267-46d1-b102-1f60bffaa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation\n",
    "def remove_punctuation(text):\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace(';', '')\n",
    "    # remove @? Might want to strip twitter usernames later\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_punctuation)\n",
    "df_valid['text'] = df_valid['text'].apply(remove_punctuation)\n",
    "df_test['text'] = df_test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0b4f85-a0c2-4810-a10e-d0cb104ce7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    sw = stopwords.words('english')\n",
    "    words = text.split(' ')\n",
    "    filtered = [w for w in words if w not in sw]\n",
    "    return ' '.join([str(v) for v in filtered])\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
    "df_valid['text'] = df_valid['text'].apply(remove_stopwords)\n",
    "df_test['text'] = df_test['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9275b-e7fd-4a13-bf7f-d269c175ec98",
   "metadata": {},
   "source": [
    "# DQ issues - missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc87737-444e-4678-8e01-e0ad71f6eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_train['text'].isna().sum())\n",
    "print(df_valid['text'].isna().sum())\n",
    "print(df_test['text'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bf70d-df29-4d94-ab9a-a0ef6fe33da2",
   "metadata": {},
   "source": [
    "# Setup transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff9b4a2-d2cd-4b8f-96f3-b2b8a804d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BERT Classifier and Tokenizer along with Input module\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3139ff7-faf9-41eb-b2e4-f753d56d1a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
